## 概率统计基础

概率论源于赌博游戏。意大利文艺复兴时代，百科全书式的学者卡尔达诺（死后）发表的《论赌博游戏》被认为是第一部论述概率论的著作。到了17世纪的法国，宫廷贵族里盛行着掷骰子游戏，游戏规则是玩家连续掷4次骰子，如果其中没有6点出现，玩家赢，如果出现一次6点，则庄家（相当于现在的赌场）赢；后来为了使游戏更刺激，游戏规则发生了些许变化，玩家用2个骰子连续掷24次，不同时出现2个6点，玩家赢，否则庄家赢。在这样的时代背景下，法国数学家帕斯卡和费马创立了概率论，后来雅各布·伯努利发现，概率论远远不止用于赌博，他将他的思考和研究记录下来，写成了《猜度数》一书，提出了大数定理（**在一个随机事件中，随着试验次数的增加，事件发生的频率越趋近于一个稳定值**），这个定理在当时的保险公司得到了充分利用。

> **思考**：回到刚才的骰子游戏，按照旧玩法，庄家获胜的概率是多少？换成新玩法，庄家获胜的概率与之前的玩法相同吗？

以概率论为基础的统计学显然比概率论出现的时间更晚，而且一直以来都是一种尴尬的存在，处于各种鄙视链的底端。从数学的角度看，统计学中的数学原理过于肤浅；从应用科学的角度看，统计学太数学，跟应用沾不上边。卢瑟福（原子核物理学之父）曾经说过，“如果你的实验需要统计学，那么你应该再做一个更好的实验”；波普尔（20世纪最伟大的哲学家之一）也曾经对归纳逻辑进行过强烈的抨击。曾经，由于欧几里得、笛卡尔带给人们的完美体系实在太过迷人，导致很多人都忽视了统计思维这一重要的科学思维方式。但是最近十年时间，随着大数据和人工智能时代的来临，统计学又以惊人的速度流行起来，因为大数据时代已经充分证明了经验主义、归纳推理的强大之处；而人工智能实际上也是大数据加上深度学习的归纳方法所取得的成功。有了统计学，我们能够处有效的处理海量的数据，也能够正确理解数据分析的结果。

按照统计方法的不同，我们可以将统计学分类为描述统计学、推断统计学、贝叶斯统计学等。描述统计学是用来描绘或总结观察量的基本情况的统计方法，通常会将整理后的数据做成表格或图表，具体包括数据的集中趋势分析、离散趋势分析和相关分析。推断统计学是研究如何根据样本数据推断总体数据特征的方法，在无法获得全量数据的情况下，推断统计就是最为行之有效的方法。贝叶斯统计学的基础是贝叶斯定理，贝叶斯定理将经验和直觉与概率相关联，和人类大脑的判断原理十分类似，简单的说就是在获取到新的数据之后，先前凭借经验和直觉获得的概率是可以改变的。

### 数据和数据的分类

在统计学中，通过试验、观察、调查等获得的材料被称为数据，数据大致可以分为定性数据和定量数据，其中定性数据又可以分为定类尺度和定序尺度，定量数据又可以分为定距尺度和定比尺度，如下表所示。

<img src="https://github.com/jackfrued/mypic/raw/master/20220320232245.png" style="zoom:50%">

####定性数据的处理

1. 定类尺度（名义尺度）：定类尺度通常会处理成虚拟变量（哑变量），多个不同的类型最终会变成一个虚拟变量矩阵。
2. 定序尺度：定序尺度可以处理成一个序号，并通过该序号表示等级的高低。

#### 定量数据的处理

1. 线性归一化
    $$ x_i' = \frac {x_i - min(X)} {max(X) - min(X)} $$

2. 零均值归一化
    $$ x_i' = \frac {x_i - \mu} {\sigma} $$

### 数据的集中趋势

我们经常会使用以下几个指标来描述一组数据的集中趋势：

1. 均值 - 均值代表某个数据集的整体水平，我们经常提到的客单价、平均访问时长、平均配送时长等指标都是均值。均值是对数据进行概括的一个强有力的方法，将大量的数据浓缩成了一个数据。均值的缺点是容易受极值的影响，可以使用加权平均值或去尾平均值来消除极值的影响；对于正数可以用几何平均值来替代算术平均值。
    - 算术平均值：$$\bar{x} = \frac{\sum_{i=1}^{n} {x_{i}}} {n} = \frac{x_{1}+x_{2}+\cdots +x_{n}}{n}$$，例如计算最近30天日均DAU、日均新增访客等，都可以使用算术平均值。
    - 几何平均值：$$\left(\prod_{i=1}^{n}x_{i}\right)^{\frac{1}{n}}={\sqrt[{n}]{x_{1}x_{2} \cdots x_{n}}}$$，例如计算不同渠道的平均转化率、不同客群的平均留存率、不同品类的平均付费率等，就可以使用几何平均值。
2. 中位数 - 将数据按照升序或降序排列后位于中间的数，它描述了数据的中等水平。中位数的计算分两种情况：
    - 当数据体量$n$为奇数时，中位数是位于$\frac{n + 1}{2}$位置的元素。
    - 当数据体量$n$为偶数时，中位数是位于$\frac{n}{2}$和${\frac{n}{2}+1}$两个位置元素的均值。
3. 众数 - 数据集合中出现频次最多的数据，它代表了数据的一般水平。一般在数据量比较大时，众数才有意义，而且数据越集中，众数的代表性就越好。众数不受极值的影响，但是无法保证唯一性和存在性。

例子：有A和B两组数据。

```
A组：5, 6, 6, 6, 6, 8, 10
B组：3, 5, 5, 6, 6, 9, 12
```

A组的均值：6.74，中位数：6，众数：6。

B组的均值：6.57，中位数：6，众数：5, 6。

> **说明**：在Excel中，可以使用`AVERAGE`、`GEOMEAN`、`MEDIAN`、`MODE.SNGL`、`MODE.MULT`函数分别计算均值、中位数和众数。求中位数也可以使用`QUARTILE.EXC`或`QUARTILE.INC`函数，将第二个参数设置为2即可。

对A组的数据进行一些调整。

```
A组：5, 6, 6, 6, 6, 8, 10, 500
B组：3, 5, 5, 6, 6, 9, 12
```

A组的均值会大幅度提升，但中位数和众数却没有变化。

|        | 优点                             | 缺点                                 |
| ------ | -------------------------------- | ------------------------------------ |
| 均值   | 充分利用了所有数据，适应性强     | 容易收到极端值（异常值）的影响       |
| 中位数 | 能够避免被极端值（异常值）的影响 | 不敏感                               |
| 众数   | 能够很好的反映数据的集中趋势     | 有可能不存在（数据没有明显集中趋势） |

### 数据的离散趋势

如果说数据的集中趋势，说明了数据最主要的特征是什么；那么数据的离散趋势，则体现了这个特征的稳定性。简单的说就是数据越集中，均值的代表性就越强；数据波动越大，均值的代表性就越弱。

1. 极值：就是最大值（maximum）、最小值（minimum），代表着数据集的上限和下限。

    > **说明**：Excel 中，计算极值的函数分别是`MAX`和`MIN`。

2. 极差：又称“全距”，是一组数据中的最大观测值和最小观测值之差，记作$R$。一般情况下，极差越大，离散程度越大，数据受极值的影响越严重。

3. 四分位距离：$\small{IQR = Q_3 - Q_1}$。

    > **提示**：箱线图。

4. 方差：将每个值与均值的偏差进行平方，然后除以总数据量得到的值。简单来说就是表示数据与期望值的偏离程度。方差越大，就意味着数据越不稳定、波动越剧烈，因此代表着数据整体比较分散，呈现出离散的趋势；而方差越小，意味着数据越稳定、波动越平滑，因此代表着数据整体比较集中。简单的总结一下，
    - 总体方差：$$ \sigma^2 = \frac {\sum_{i=1}^{N} {(X_i - \mu)^2}} {N} $$。
    - 样本方差：$$ S^2 = \frac {\sum_{i=1}^{N} {(X_i - \bar{X})^2}} {N-1} $$。

    > **说明**：Excel 中，计算总体方差和样本方差的函数分别是`VAR.P`和`VAR.S`。

5. 标准差：将方差进行平方根运算后的结果，与方差一样都是表示数据与期望值的偏离程度。
    - 总体标准差：$$ \sigma = \sqrt{\frac{\sum_{i=1}^{N} {(X_i - \mu)^2}} {N}} $$
    - 样本标准差：$$ S = \sqrt{\frac{\sum_{i=1}^{N} {(X_i - \bar{X})^2}} {N-1}} $$

    > **说明**：Excel 中，计算标准差的函数分别是`STDEV.P`和`STDEV.S`。

### 数据的频数分析

用一定的方式将数据分组，然后统计每个分组中样本的数量，再辅以图表（如直方图）就可以更直观的展示数据分布趋势的一种方法。

频数分析的业务意义：

1. 大问题变小问题，迅速聚焦到需要关注的群体。
2. 找到合理的分类机制，有利于长期的数据分析（维度拆解）。

例如：一个班有50个学生，考试成绩如下所示：

```
87,  80,  79,  78,  55,  80,  81,  60,  78,  82,  67,  74,  67, 74,  66,  91,  100,  70,  82,  71,  77,  94,  75,  83,  85,  84, 47,  75,  84,  96,  53,  86,  86,  89,  71,  76,  75,  80,  70,  83,  77,  91,  90,  82,  74,  74,  78,  53,  88,  72
```

在获得数据后，我们先解读数据的集中趋势和离散趋势。

均值：`77.4`，中位数：`78.0`，众数：`74`。

最高分：`100`，最低分：`47`，极差：`53`，方差：`120.16`。

但是，仅仅依靠上面的指标是很难对一个数据集做出全面的解读，我们可以把学生按照考试成绩进行分组，如下所示。

| 分数段   | 学生人数 |
| -------- | -------- |
| <60      | 4        |
| [60, 65) | 1        |
| [65, 70) | 3        |
| [70, 75) | 9        |
| [75, 80) | 10       |
| [80, 85) | 11       |
| [85, 90) | 6        |
| [90, 95) | 4        |
| >=95     | 2        |

我们可以利用直方图来查看数据分布的形态，对数据分布形态的测度主要以正态分布为标准进行衡量，正态分布在坐标轴上的形状是一个铃铛型（钟型），正态曲线以均值为中心左右对称，如下图所示，而上面的学生考试成绩数据就呈现出正态分布的轮廓。

<img src="https://github.com/jackfrued/mypic/raw/master/20210716155507.png" width="80%">

我们可以数据分布的直方图拟合出一条曲线与正态曲线进行比较，主要比较曲线的尖峭程度和对称性，通常称之为峰度和偏态。数据分布的不对称性称为偏态，偏态又分为正偏（右偏）或负偏（左偏）两种。在正态分布的情况下，中位数和均值应该都在对称轴的位置，如果中位数在左边，均值在右边，那么数据的极端值也在右边，数据分布曲线向右延伸，就是我们说的右偏；如果均值在左边，中位数在右边，那么数据的极端值在左边，数据分布曲线向左延伸，就是我们说的左偏。测定偏态的指标是偏态系数，Excel 中计算偏度系数使用的公式如下所示：
$$
SK = \frac{n}{(n - 1)(n - 2)} \sum(\frac{x_i - \bar{x}}{s})^3
$$
$\small{SK > 0}$时，分布呈现正偏，SK值越大，正偏程度越高。

$\small{SK < 0}$时，分布呈现负偏，SK值越小，负偏程度越高。

峰度是指数据分布的尖峭程度，一般可以表现为尖顶峰度、平顶峰度和标准峰度（正态分布的峰度）。测定峰度的指标是峰度系数，Excel 中计算峰度系数使用的公式如下所示：
$$
K = \frac{n(n + 1)}{(n - 1)(n - 2)(n - 3)}\sum(\frac{x_i - \bar{x}}{s})^4-\frac{3(n - 1)^2}{(n - 2)(n - 3)}
$$
峰度系数$\small{K < 0}$时，分布与正态分布相比更为扁平、宽肩、瘦尾；峰度系数$\small{K > 0}$时，分布与正态分布相比更为尖峰、瘦肩、肥尾。

### 数据的概率分布

#### 基本概念

1. 随机现象：在一定条件下可能发生也可能不发生，结果具有偶然性的现象。

2. 样本空间（*sample space*）：随机现象一切可能的结果组成的集合。

  - 抛一枚硬币的样本空间：$\Omega = \{ \omega_1, \omega_2 \}$。
  - 抛两枚硬币的样本空间：$\Omega = \{ \omega_1, \omega_2, \omega_3, \omega_4 \}$，其中$\omega_1 = (H, H)$，$\omega_2 = (H, T)$，$\omega_3 = (T, H)$，$\omega_4 = (T, T)$。
  - 离散型的样本空间的元素是可列的，连续型的样本空间的元素是（无限）不可列的。

3. 随机试验（*trials*）：在相同条件下对某种随机现象进行观测的试验。随机试验满足三个特点：

    - 可以在相同条件下重复的进行。

    - 每次试验的结果不止一个，事先可以明确指出全部可能的结果。

    - 重复试验的结果以随机的方式出现（事先不确定会出现哪个结果）。

    <img src="https://github.com/jackfrued/mypic/raw/master/20220322075000.png" style="zoom:75%">

4. 随机变量（*random variable*）：如果$X$指定给概率空间$S$中每一个事件$e$有一个实数$X(e)$，同时针对每一个实数$r$都有一个事件集合$A_r$与其相对应，其中$A_r=\{e: X(e) \le r\}$，那么$X$被称作随机变量。从这个定义看出，$X$的本质是一个实值函数，以给定事件为自变量的实值函数，因为函数在给定自变量时会产生因变量，所以将$X$称为随机变量。简单的说，随机变量的值需要通过试验来确认。

    - 离散型随机变量：数据可以一一列出。
    - 连续型随机变量：数据不可以一一列出。

    > **说明**：如果离散型随机变量的取值非常庞大时，可以近似看做连续型随机变量。

    <img src="https://github.com/jackfrued/mypic/raw/master/20220322075148.png" style="zoom:50%;">

    <img src="https://github.com/jackfrued/mypic/raw/master/20220322075331.png" style="zoom:50%;">

5. 概率（*probability*）：用一个0~1之间的数字表示随机现象发生的可能性，也就是说概率是随机事件出现可能性的度量。

6. 概率质量函数/概率密度函数：概率质量函数是描述离散型随机变量为特定取值的概率的函数，通常缩写为**PMF**。概率密度函数是描述连续型随机变量在某个确定的取值点可能性的函数，通常缩写为**PDF**。二者的区别在于，概率密度函数本身不是概率，只有对概率密度函数在某区间内进行积分后才是概率。

7. 随机变量的数字特征：

    - （数学）期望：随机变量按照概率的加权平均，它表示了概率分布的中心位置，反映随机变量平均取值的大小。

        对于离散型随机变量$ X $，若$ \sum_{i=1}^{\infty} x_ip_i $收敛，那么它就是随机变量$ X $的期望，记为$ E(X) $，即$ E(X) = \sum_{i=1}^{\infty} x_ip_i $，否则随机变量$ X $的期望不存在。

        对于连续型随机变量$ X $，其概率密度函数为$ f(x) $，若$ \int_{-\infty}^{\infty}xf(x)dx $收敛，则称$ E(x) =  \int_{-\infty}^{\infty}xf(x)dx $为随机变量$ X $的数学期望，否则随机变量$ X $的期望不存在。

    - 方差：方差用来表示随机变量概率分布的离散程度，对于随机变量$ X $，若$ E((X - E(X))^2) $存在，则称$ E((X - E(X))^2) $为$ X $的方差，记为$ Var(X) $。很显然，离散型随机变量$ X $的方差为$ Var(X) = \sum_{i=1}^{\infty} [x_i - E(X)]^2p_i$，连续型随机变量$ X $的方差为$ Var(X) = \int_{-\infty}^{\infty} [x - E(X)]^2f(x)dx $。

8. 期望与方差的性质：

    - 对于任意两个随机变量$ X_1 $和$ X_2 $，则有$ E(X_1 + X_2) = E(X_1) + E(X_2) $。
    - 若$ X $是随机变量，$ a $和$ b $是任意常量，则有$ E(aX + b) = aE(X) + b $和$ Var(aX + b) = a^2Var(X)$。
    - 若随机变量$ X_1 $和$ X_2 $独立，则有$ Var(X_1 + X_2) = Var(X_1) + Var(X_2) $。

9. 其他零碎小概念：

    - 互斥（*mutually exclusive*）：事件不能同时发生。
    - 独立（*independant*）：一个试验的结果不会对另一个试验的结果产生影响。
    - 排列（permutation）：$ P_k^n = \frac{n!}{(n - k)!} $，国内教科书一般记为$ P_n^k $或$ A_n^k $。
    - 组合（*combination*）：$ C_k^n = \frac{n!}{k!(n-k)!} $，国内教科书一般记为$ C_n^k $。

#### 离散型分布

1. 伯努利分布（*Bernoulli distribution*）：又名**两点分布**或者**0-1分布**，是一个离散型概率分布。若伯努利试验成功，则随机变量取值为1。若伯努利试验失败，则随机变量取值为0。记其成功概率为$ p (0 \le p \le 1) $，失败概率为$ q=1-p $，则概率质量函数为：

    $$ f(x)=p^{x}(1-p)^{1-x}=\left\{{\begin{matrix}p&{\mbox{if }}x=1,\\q\ &{\mbox{if }}x=0.\\\end{matrix}}\right. $$

2. 二项分布（*Binomial distribution*）：$n$个独立的是/非试验中成功次数的离散概率分布，其中每次试验的成功概率为$p$。一般地，如果随机变量$X$服从参数为$ n $和$ p $的二项分布，记为$ X\sim B(n,p) $。$ n $次试验中正好得到$ k $次成功的概率由概率质量函数给出，
    $$ P(X=k) = C_k^np^k(1-p)^{n-k} $$

    > **提示**：Excel 中，可以通过`BINOM.DIST.RANGE`函数计算二项分布的概率。

3. 泊松分布（*Poisson distribution*）：适合于描述单位时间内随机事件发生的次数的概率分布。如某一服务设施在一定时间内受到的服务请求的次数、汽车站台的候客人数、机器出现的故障数、自然灾害发生的次数、DNA序列的变异数、放射性原子核的衰变数等等。泊松分布的概率质量函数为：$P(X=k)=\frac{e^{-\lambda}\lambda^k}{k!}$，泊松分布的参数$\lambda$是单位时间（或单位面积）内随机事件的平均发生率。

    > **说明**：泊松分布是在没有计算机的年代，由于二项分布的运算量太大运算比较困难，为了减少运算量，数学家为二项分布提供的一种近似。当二项分布的$n$很大，$p$很小的时候，我们可以让$\lambda = np$，然后用泊松分布的概率质量函数计算概率来近似二项分布的概率。

#### 分布函数

对于连续型随机变量，我们不可能去罗列每一个值出现的概率，因此要引入分布函数的概念。
$$
F(x) = P\{X \le x\}
$$
如果将$ X $看成是数轴上的随机坐标，上面的分布函数表示了$ x $落在区间$ (-\infty, x) $中的概率。分布函数有以下性质：

1. $ F(x) $是一个单调不减的函数；
2. $ 0 \le F(x) \le 1$，且$ F(-\infty) = \lim_{x \to -\infty} F(x) = 0 $， $F(\infty) = \lim_{x \to \infty} F(x) = 1$；
3. $ F(x) $是右连续的。

概率密度函数就是给分布函数求导的结果，简单的说就是：
$$
F(x) = \int_{- \infty}^{x} f(t)dt
$$

#### 连续型分布

1. 均匀分布（*Uniform distribution*）：如果连续型随机变量$X$具有概率密度函数$f(x)=\begin{cases}{\frac{1}{b-a}} \quad &{a \leq x \leq b} \\ {0} \quad &{\mbox{other}}\end{cases}$，则称$X$服从$[a,b]$上的均匀分布，记作$X\sim U[a,b]$。

2. 指数分布（*Exponential distribution*）：如果连续型随机变量$X$具有概率密度函数$f(x)=\begin{cases} \lambda e^{- \lambda x} \quad &{x \ge 0} \\ {0} \quad &{x \lt 0} \end{cases}$，则称$X$服从参数为$\lambda$的指数分布，记为$X \sim Exp(\lambda)$。指数分布可以用来表示独立随机事件发生的时间间隔，比如旅客进入机场的时间间隔、客服中心接入电话的时间间隔、知乎上出现新问题的时间间隔等等。指数分布的一个重要特征是无记忆性（无后效性），这表示如果一个随机变量呈指数分布，它的条件概率遵循：$P(T \gt s+t\ |\ T \gt t)=P(T \gt s), \forall s,t \ge 0$。

3. 正态分布（*Normal distribution*）：又名**高斯分布**（*Gaussian distribution*），是一个非常常见的连续概率分布，经常用自然科学和社会科学中来代表一个不明的随机变量。若随机变量$X$服从一个位置参数为$\mu$、尺度参数为$\sigma$的正态分布，记为$X \sim N(\mu,\sigma^2)$，其概率密度函数为：$\displaystyle f(x)={\frac {1}{{\sqrt {2 \pi } \sigma}}}e^{-{\frac {\left(x-\mu \right)^{2}}{2\sigma ^{2}}}}$。

    根据“棣莫弗-拉普拉斯积分定理”，假设$ \mu_{n} (n=1, 2, \cdots) $表示$ n $重伯努利试验中成功的次数，已知每次试验成功的概率为$p$，那么：
    $$ \lim_{n \to \infty} P\lbrace \frac{\mu_n - np} {\sqrt{np(1-p)}} \le x \rbrace = \frac {1} {\sqrt{2\pi}} \int_{-\infty}^{x} e^{-\frac {\mu^2} {2}} dx $$，该定理表明正态分布是二项分布的极限分布。
    

提到正态分布，就必须说一下“3$\sigma$法则”，该法则也称为“68-95-99.7”法则，如下图所示。

<img src="https://github.com/jackfrued/mypic/raw/master/20210716155542.png" style="zoom:65%">

正态分布有一个非常重要的性质，**大量统计独立的随机变量的平均值的分布趋于正态分布**，即$ \bar{X} \sim N(\mu, \frac{\sigma^2}{n}) $这就是**中心极限定理**。中心极限定理的重要意义在于，我们可以用正态分布作为其他概率分布的近似。

一个例子：假设某校入学新生的智力测验平均分数与标准差分别为 100 与 12。那么随机抽取 50 个学生，他们智力测验平均分数大于 105 的概率是多少？小于 90 的概率是多少？

本例没有正态分布的假设，还好中心极限定理提供一个可行解，那就是当随机样本数量超过30，样本平均数近似于一个正态变量，我们可以构造标准正态变量$ Z = \frac {\bar{X} - \mu} {\sigma / \sqrt{n}} $。

平均分数大于 105 的概率为：$ P(Z \gt \frac{105 - 100}{12 / \sqrt{50}}) = P(Z \gt 5/1.7) = P(Z \gt 2.94) = 0.0016$。

平均分数小于 90 的概率为：$ P(Z \lt \frac{90-100}{12/\sqrt{50}}) = P(Z < -5.88) = 0.0000 $。
    

> **说明**：上面标准正态分布的概率值可以查表得到，在 Excel 中可以使用`NORM.DIST`函数获得。例如在上面的例子中，我们可以通过`NORM.DIST(2.94, 0, 1, TRUE)`获得$P(z\le2.94)$的概率为`0.998359`。

#### 基于正态分布的三大分布

1. 卡方分布（*Chi-square distribution*）：若$k$个随机变量$Z_1,Z_2,...,Z_k$是相互独立且服从标准正态分布$N(0, 1)$的随机变量，则随机变量$X = \sum_{i=1}^{k}Z_i^2$被称为服从自由度为$k$的卡方分布，记为$X \sim \chi^2(k)$。卡方分布的概率密度曲线如下所示。

    <img src="https://github.com/jackfrued/mypic/raw/master/20220323201608.png" style="zoom:50%;">

2. $t$分布：设$X \sim N(0, 1)$， $Y \sim {\chi}^2(n)$，且$X$与$Y$相互独立，则随机变量$T = \frac {X} {\sqrt{Y/n}}$称为自由度为$n$的$t$分布，记作$T \sim t(n)$。$t$分布的概率密度曲线如下所示。

    <img src="https://github.com/jackfrued/mypic/raw/master/20220323203530.png" style="zoom:50%">

3. $F$分布：设$X \sim \chi^2(n_1)$，$Y \sim \chi^2(n_2)$，且$X$与$Y$相互独立，则随机变量$F = \frac{X / n_1}{Y / n_2}$称为自由度为$(n_1, n_2)$的$F$分布，记作$F \sim F(n_1, n_2)$，它的概率密度曲线如下所示。

    <img src="https://github.com/jackfrued/mypic/raw/master/20220619164716.png" style="zoom: 50%;">

这三个分布有什么用呢？

1. $ \chi^2 $分布：常用于独立性检验、拟合优度检验。
2. $ F $分布：常用于比例的估计和检验，方差分析和回归分析中也会用到$ F $分布。
3. $ t $分布：在信息不足的情况下，要对总体均值进行估计和检验，就会使用到$ t $分布。

### 其他内容

#### 贝叶斯定理

**联合概率**是指事件A和事件B共同发生的概率，通常记为$\small{P(A \cap B)}$。

**条件概率**是指事件A在事件B发生的条件下发生的概率，通常记为$\small{P(A|B)}$。设A与B为样本空间$\Omega$中的两个事件，其中$\small{P(B) \gt 0}$。那么在事件B发生的条件下，事件A发生的条件概率为：${P(A|B)=\frac{P(A \cap B)}{P(B)}}$，当$ P(B)=0 $时，规定$ P(A|B) = 0 $。

> **思考**：
>
> 1. 某家庭有两个孩子，问两个孩子都是女孩的概率是多少？
> 2. 某家庭有两个孩子，已知其中一个是女孩，问两个孩子都是女孩的概率是多少？
> 3. 某家庭有两个孩子，已知老大是女孩，问两个孩子都是女孩的概率是多少？

事件A在事件B已发生的条件下发生的概率，与事件B在事件A已发生的条件下发生的概率是不一样的。然而，这两者是有确定的关系的，**贝叶斯定理**就是对这种关系的陈述，如下所示：
$$
P(A|B)=\frac{P(B|A)}{P(B)}P(A)
$$

- $P(A|B)$是已知$B$发生后，$A$的条件概率，也称为$A$的后验概率。
- $P(A)$是$A$的先验概率也称作边缘概率，是不考虑$B$时$A$发生的概率。
- $P(B|A)$是已知$A$发生后，$B$的条件概率，称为$B$的似然性。
- $P(B)$是$B$的先验概率。

#### 大数定理

样本数量越多，则其算术平均值就有越高的概率接近期望值。

1. 弱大数定律（辛钦定理）：样本均值依概率收敛于期望值，即对于任意正数$\epsilon$，有：$\lim_{n \to \infty}P(|\bar{X_n}-\mu|>\epsilon)=0$。
2. 强大数定律：样本均值以概率1收敛于期望值，即：$P(\lim_{n \to \infty}\bar{X_n}=\mu)=1$。

#### 假设检验

假设检验就是通过抽取样本数据，并且通过**小概率反证法**去验证整体情况的方法。假设检验的核心思想是小概率反证法（首先假设想推翻的命题是成立的，然后试图找出矛盾，找出不合理的地方来证明命题为假命题），即在**零假设**（通常记为$H_0$）的前提下，估算某事件发生的可能性，如果该事件是小概率事件，在一次试验中本不应该发生，但现在却发生了，此时我们就有足够的理由怀疑零假设，转而接受**备择假设**（通常记为$H_A$）。

假设检验会存在两种错误情况，一种称为“拒真”，一种称为“取伪”。如果原假设是对的，但你拒绝了原假设，这种错误就叫作“拒真”，这个错误的概率也叫作显著性水平$\alpha$，或称为容忍度；如果原假设是错的，但你承认了原假设，这种错误就叫作“取伪”，这个错误的概率我们记为$\beta$。

### 总结

描述性统计通常用于研究表象，将现象用数据的方式描述出来（用整体的数据来描述整体的特征）；推理性统计通常用于推测本质（通过样本数据特征去推理总体数据特征），也就是你看到的表象的东西有多大概率符合你对隐藏在表象后的本质的猜测。
